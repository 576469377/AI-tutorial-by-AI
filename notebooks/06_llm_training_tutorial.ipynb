{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Training Tutorial\n",
    "\n",
    "Welcome to the comprehensive tutorial on training Large Language Models! This notebook will guide you through:\n",
    "\n",
    "1. **Understanding Transformers** - The architecture behind modern LLMs\n",
    "2. **Building from Scratch** - Implementing a simple language model\n",
    "3. **Training Process** - Learning how to train your own model\n",
    "4. **Text Generation** - Using your model to generate text\n",
    "5. **Fine-tuning** - Adapting pre-trained models\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Transformer Architecture\n",
    "\n",
    "Let's start by implementing the core components of a transformer model:\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "The attention mechanism is the heart of transformers. It allows the model to focus on different parts of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads and project\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        return self.w_o(attention_output)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        return torch.matmul(attention_weights, V)\n",
    "\n",
    "# Test the attention mechanism\n",
    "attention = MultiHeadAttention(d_model=128, num_heads=8)\n",
    "x = torch.randn(2, 10, 128)  # batch_size=2, seq_len=10, d_model=128\n",
    "output = attention(x, x, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úÖ Multi-head attention working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "Now let's build a complete transformer block that combines attention with a feed-forward network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with self-attention and feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attended = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attended))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        fed_forward = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(fed_forward))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the transformer block\n",
    "transformer_block = TransformerBlock(d_model=128, num_heads=8, d_ff=512)\n",
    "x = torch.randn(2, 10, 128)\n",
    "output = transformer_block(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úÖ Transformer block working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Simple Language Model\n",
    "\n",
    "Now let's create a complete language model using our transformer components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create a causal mask to prevent attention to future positions\"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    return mask == 0  # True for allowed positions, False for masked\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"A simple transformer-based language model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=4, \n",
    "                 d_ff=1024, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layers\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        pos_embeds = self.position_embedding(positions)\n",
    "        x = token_embeds + pos_embeds\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = create_causal_mask(seq_len).to(input_ids.device)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, causal_mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create a model\n",
    "vocab_size = 1000  # We'll build a proper vocabulary later\n",
    "model = SimpleLanguageModel(vocab_size=vocab_size, d_model=256, num_heads=8, num_layers=4)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {total_params:,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, vocab_size, (2, 20))  # batch_size=2, seq_len=20\n",
    "output = model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úÖ Language model working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization and Data Preparation\n",
    "\n",
    "Before we can train our model, we need to convert text into tokens that the model can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"Simple word-level tokenizer for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        self.bos_token = '<BOS>'\n",
    "        \n",
    "        # Add special tokens\n",
    "        self._add_word(self.pad_token)\n",
    "        self._add_word(self.unk_token)\n",
    "        self._add_word(self.eos_token)\n",
    "        self._add_word(self.bos_token)\n",
    "        \n",
    "        self.pad_token_id = self.word_to_id[self.pad_token]\n",
    "        self.unk_token_id = self.word_to_id[self.unk_token]\n",
    "        self.eos_token_id = self.word_to_id[self.eos_token]\n",
    "        self.bos_token_id = self.word_to_id[self.bos_token]\n",
    "    \n",
    "    def _add_word(self, word):\n",
    "        if word not in self.word_to_id:\n",
    "            self.word_to_id[word] = self.vocab_size\n",
    "            self.id_to_word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    def build_vocab(self, texts, min_freq=1):\n",
    "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Add words that meet minimum frequency threshold\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= min_freq:\n",
    "                self._add_word(word)\n",
    "        \n",
    "        print(f\"Built vocabulary with {self.vocab_size} tokens\")\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"Convert text to list of token IDs\"\"\"\n",
    "        words = text.lower().split()\n",
    "        token_ids = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            token_ids.append(self.bos_token_id)\n",
    "        \n",
    "        for word in words:\n",
    "            token_id = self.word_to_id.get(word, self.unk_token_id)\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            token_ids.append(self.eos_token_id)\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        \"\"\"Convert list of token IDs back to text\"\"\"\n",
    "        words = []\n",
    "        for token_id in token_ids:\n",
    "            word = self.id_to_word.get(token_id, self.unk_token)\n",
    "            if skip_special_tokens and word in [self.pad_token, self.unk_token, self.eos_token, self.bos_token]:\n",
    "                continue\n",
    "            words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Create sample text data\n",
    "sample_texts = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"machine learning is a subset of artificial intelligence\",\n",
    "    \"neural networks are inspired by biological neural networks\", \n",
    "    \"deep learning uses multiple layers to learn representations\",\n",
    "    \"transformers use attention mechanisms for better performance\",\n",
    "    \"language models predict the next word in a sequence\",\n",
    "    \"artificial intelligence will transform many industries\",\n",
    "    \"data science combines statistics programming and domain knowledge\",\n",
    "    \"python is a popular programming language for machine learning\",\n",
    "    \"the future of technology depends on continued innovation\"\n",
    "]\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(sample_texts)\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"machine learning is fascinating\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded text: {decoded}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and Training Setup\n",
    "\n",
    "Let's create a dataset class and set up the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for language modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        \n",
    "        for text in texts:\n",
    "            token_ids = tokenizer.encode(text)\n",
    "            \n",
    "            # Split long texts into chunks\n",
    "            for i in range(0, len(token_ids) - max_length + 1, max_length // 2):\n",
    "                chunk = token_ids[i:i + max_length]\n",
    "                if len(chunk) == max_length:\n",
    "                    self.examples.append(chunk)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "\n",
    "# Create expanded dataset for better training\n",
    "expanded_texts = sample_texts * 50  # Repeat texts to have more training data\n",
    "\n",
    "# Split into train and validation\n",
    "split_idx = int(0.8 * len(expanded_texts))\n",
    "train_texts = expanded_texts[:split_idx]\n",
    "val_texts = expanded_texts[split_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_length=32)\n",
    "val_dataset = TextDataset(val_texts, tokenizer, max_length=32)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Show a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample text: {tokenizer.decode(sample_batch[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Language Model\n",
    "\n",
    "Now let's train our language model! We'll track the loss and visualize the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with the correct vocabulary size\n",
    "model = SimpleLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_layers=3,\n",
    "    d_ff=512,\n",
    "    max_seq_len=64\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {total_params:,} parameters\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # For language modeling, targets are input shifted by one position\n",
    "        inputs = batch[:, :-1]\n",
    "        targets = batch[:, 1:]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            epoch_val_loss += loss.item()\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_train_loss = epoch_train_loss / num_batches\n",
    "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Training Progress\n",
    "\n",
    "Let's plot the training and validation losses to see how our model learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Language Model Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate perplexity (lower is better)\n",
    "final_train_perplexity = math.exp(train_losses[-1])\n",
    "final_val_perplexity = math.exp(val_losses[-1])\n",
    "\n",
    "print(f\"Final Training Perplexity: {final_train_perplexity:.2f}\")\n",
    "print(f\"Final Validation Perplexity: {final_val_perplexity:.2f}\")\n",
    "\n",
    "# Show training statistics\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"Initial Train Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Loss Reduction: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Generation\n",
    "\n",
    "Now for the exciting part - let's use our trained model to generate text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_k=10):\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt, add_special_tokens=False), \n",
    "                           dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get model predictions\n",
    "            logits = model(generated_tokens)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n",
    "                next_token_logits[top_k_indices] = top_k_logits\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            \n",
    "            # Stop if we generate an end-of-sequence token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    # Decode and return generated text\n",
    "    generated_text = tokenizer.decode(generated_tokens[0].tolist(), skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test text generation with different prompts\n",
    "test_prompts = [\n",
    "    \"machine learning\",\n",
    "    \"the future of\",\n",
    "    \"artificial intelligence\",\n",
    "    \"neural networks\",\n",
    "    \"deep learning\"\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Text Generation Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=15, temperature=0.8, top_k=10)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: '{generated}'\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Try with different temperatures\n",
    "print(\"\\nüå°Ô∏è Temperature Effects on Generation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompt = \"artificial intelligence\"\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=15, temperature=temp, top_k=10)\n",
    "    print(f\"Temperature {temp}: '{generated}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance using perplexity and other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_detailed_perplexity(model, data_loader):\n",
    "    \"\"\"Calculate perplexity with detailed statistics\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss, total_tokens\n",
    "\n",
    "# Calculate perplexity for train and validation sets\n",
    "train_perplexity, train_loss, train_tokens = calculate_detailed_perplexity(model, train_loader)\n",
    "val_perplexity, val_loss, val_tokens = calculate_detailed_perplexity(model, val_loader)\n",
    "\n",
    "print(\"üìä Model Evaluation Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  Perplexity: {train_perplexity:.2f}\")\n",
    "print(f\"  Loss: {train_loss:.4f}\")\n",
    "print(f\"  Tokens evaluated: {train_tokens:,}\")\n",
    "print()\n",
    "print(f\"Validation Set:\")\n",
    "print(f\"  Perplexity: {val_perplexity:.2f}\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  Tokens evaluated: {val_tokens:,}\")\n",
    "print()\n",
    "\n",
    "# Model size and efficiency metrics\n",
    "model_size_mb = total_params * 4 / (1024 * 1024)  # Assuming float32\n",
    "print(f\"Model Statistics:\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  Model size: {model_size_mb:.1f} MB\")\n",
    "print(f\"  Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Max sequence length: {model.max_seq_len}\")\n",
    "\n",
    "# Analyze training efficiency\n",
    "improvement = (train_losses[0] - train_losses[-1]) / train_losses[0] * 100\n",
    "print(f\"\\nTraining Efficiency:\")\n",
    "print(f\"  Initial loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Final loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.1f}%\")\n",
    "print(f\"  Epochs trained: {len(train_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fine-tuning with Transformers Library\n",
    "\n",
    "Now let's see how to fine-tune a pre-trained model using the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell demonstrates fine-tuning with transformers library\n",
    "# Uncomment and run if you have transformers installed\n",
    "\n",
    "# try:\n",
    "#     from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "#     \n",
    "#     print(\"ü§ó Fine-tuning with Transformers Library\")\n",
    "#     print(\"=\" * 50)\n",
    "#     \n",
    "#     # Load a pre-trained model (small for demo)\n",
    "#     model_name = \"gpt2\"\n",
    "#     pretrained_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#     pretrained_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     \n",
    "#     # Add padding token\n",
    "#     if pretrained_tokenizer.pad_token is None:\n",
    "#         pretrained_tokenizer.pad_token = pretrained_tokenizer.eos_token\n",
    "#     \n",
    "#     print(f\"Loaded pre-trained model: {model_name}\")\n",
    "#     print(f\"Model parameters: {sum(p.numel() for p in pretrained_model.parameters()):,}\")\n",
    "#     print(f\"Vocabulary size: {len(pretrained_tokenizer)}\")\n",
    "#     \n",
    "#     # Generate some text with the pre-trained model\n",
    "#     prompt = \"Artificial intelligence is\"\n",
    "#     inputs = pretrained_tokenizer.encode(prompt, return_tensors='pt')\n",
    "#     \n",
    "#     with torch.no_grad():\n",
    "#         outputs = pretrained_model.generate(\n",
    "#             inputs, \n",
    "#             max_length=50, \n",
    "#             temperature=0.8, \n",
    "#             do_sample=True,\n",
    "#             pad_token_id=pretrained_tokenizer.eos_token_id\n",
    "#         )\n",
    "#     \n",
    "#     generated_text = pretrained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     print(f\"\\nPre-trained model generation:\")\n",
    "#     print(f\"Prompt: '{prompt}'\")\n",
    "#     print(f\"Generated: '{generated_text}'\")\n",
    "#     \n",
    "# except ImportError:\n",
    "#     print(\"Transformers library not available.\")\n",
    "#     print(\"Install with: pip install transformers\")\n",
    "\n",
    "print(\"üí° Fine-tuning Tips:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"1. Start with a pre-trained model for better results\")\n",
    "print(\"2. Use smaller learning rates for fine-tuning (1e-5 to 5e-5)\")\n",
    "print(\"3. Fine-tune for fewer epochs to avoid overfitting\")\n",
    "print(\"4. Use domain-specific data for better task performance\")\n",
    "print(\"5. Consider parameter-efficient methods like LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Load Your Model\n",
    "\n",
    "Let's save our trained model so we can use it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model_save_path = 'my_language_model.pth'\n",
    "tokenizer_save_path = 'my_tokenizer.json'\n",
    "\n",
    "# Save model state\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'd_model': 128,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 3,\n",
    "        'd_ff': 512,\n",
    "        'max_seq_len': 64\n",
    "    },\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'final_perplexity': val_perplexity\n",
    "}, model_save_path)\n",
    "\n",
    "# Save tokenizer\n",
    "import json\n",
    "tokenizer_data = {\n",
    "    'word_to_id': tokenizer.word_to_id,\n",
    "    'id_to_word': tokenizer.id_to_word,\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'special_tokens': {\n",
    "        'pad_token': tokenizer.pad_token,\n",
    "        'unk_token': tokenizer.unk_token,\n",
    "        'eos_token': tokenizer.eos_token,\n",
    "        'bos_token': tokenizer.bos_token\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(tokenizer_save_path, 'w') as f:\n",
    "    json.dump(tokenizer_data, f)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_save_path}\")\n",
    "print(f\"‚úÖ Tokenizer saved to: {tokenizer_save_path}\")\n",
    "\n",
    "# Demonstrate loading the model\n",
    "def load_model_and_tokenizer(model_path, tokenizer_path):\n",
    "    \"\"\"Load a saved model and tokenizer\"\"\"\n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    config = checkpoint['model_config']\n",
    "    \n",
    "    # Create model with saved configuration\n",
    "    loaded_model = SimpleLanguageModel(**config)\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loaded_model.to(device)\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    with open(tokenizer_path, 'r') as f:\n",
    "        tokenizer_data = json.load(f)\n",
    "    \n",
    "    loaded_tokenizer = SimpleTokenizer()\n",
    "    loaded_tokenizer.word_to_id = tokenizer_data['word_to_id']\n",
    "    loaded_tokenizer.id_to_word = {int(k): v for k, v in tokenizer_data['id_to_word'].items()}\n",
    "    loaded_tokenizer.vocab_size = tokenizer_data['vocab_size']\n",
    "    \n",
    "    return loaded_model, loaded_tokenizer\n",
    "\n",
    "# Test loading\n",
    "print(\"\\nüîÑ Testing model loading...\")\n",
    "loaded_model, loaded_tokenizer = load_model_and_tokenizer(model_save_path, tokenizer_save_path)\n",
    "\n",
    "# Test generation with loaded model\n",
    "test_prompt = \"machine learning\"\n",
    "generated = generate_text(loaded_model, loaded_tokenizer, test_prompt, max_length=10)\n",
    "print(f\"Loaded model generation: '{generated}'\")\n",
    "print(\"‚úÖ Model loading successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Bonus: Introduction to Multimodal AI\n",
    "\n",
    "Now that you've mastered language models, let's explore the exciting world of **Multimodal AI** - models that understand both text and images!\n",
    "\n",
    "### What are Multimodal Models?\n",
    "Multimodal models can process and understand multiple types of data:\n",
    "- **Text + Images**: Image captioning, visual question answering\n",
    "- **Text + Audio**: Speech recognition, text-to-speech\n",
    "- **Text + Video**: Video understanding and description\n",
    "\n",
    "Let's explore some practical examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's try CLIP - a powerful vision-language model\n",
    "try:\n",
    "    from transformers import CLIPProcessor, CLIPModel\n",
    "    from PIL import Image\n",
    "    import requests\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    print(\"üîó Loading CLIP model...\")\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    print(\"‚úÖ CLIP model loaded successfully!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è CLIP not available. Install transformers: pip install transformers\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CLIP: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a simple vision-language model architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleVisionLanguageModel(nn.Module):\n",
    "    \"\"\"A simple model that combines vision and text understanding\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Vision encoder (simplified CNN)\n",
    "        self.vision_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((8, 8)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, d_model)\n",
    "        )\n",
    "        \n",
    "        # Text encoder (using our transformer components)\n",
    "        self.text_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.text_encoder = TransformerBlock(d_model, num_heads=8, d_ff=d_model*4)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.cross_attention = nn.MultiheadAttention(d_model, num_heads=8)\n",
    "        \n",
    "        # Output projections\n",
    "        self.image_projection = nn.Linear(d_model, d_model)\n",
    "        self.text_projection = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, image, text):\n",
    "        # Encode image\n",
    "        image_features = self.vision_encoder(image)  # [batch, d_model]\n",
    "        \n",
    "        # Encode text\n",
    "        text_embeds = self.text_embedding(text)      # [batch, seq_len, d_model]\n",
    "        text_features = self.text_encoder(text_embeds, mask=None)\n",
    "        text_features = text_features.mean(dim=1)    # [batch, d_model]\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        image_features = image_features.unsqueeze(1)  # [batch, 1, d_model]\n",
    "        text_features_expanded = text_features.unsqueeze(1)  # [batch, 1, d_model]\n",
    "        \n",
    "        attended_features, _ = self.cross_attention(\n",
    "            image_features, text_features_expanded, text_features_expanded\n",
    "        )\n",
    "        \n",
    "        # Project to common space\n",
    "        image_proj = self.image_projection(attended_features.squeeze(1))\n",
    "        text_proj = self.text_projection(text_features)\n",
    "        \n",
    "        return image_proj, text_proj\n",
    "\n",
    "# Create the multimodal model\n",
    "multimodal_model = SimpleVisionLanguageModel(vocab_size=1000)\n",
    "total_params = sum(p.numel() for p in multimodal_model.parameters())\n",
    "\n",
    "print(f\"üé® Multimodal Model Created!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Test with dummy data\n",
    "dummy_image = torch.randn(2, 3, 224, 224)  # Batch of 2 images\n",
    "dummy_text = torch.randint(0, 1000, (2, 10))  # Batch of 2 text sequences\n",
    "\n",
    "image_proj, text_proj = multimodal_model(dummy_image, dummy_text)\n",
    "print(f\"Image projection shape: {image_proj.shape}\")\n",
    "print(f\"Text projection shape: {text_proj.shape}\")\n",
    "print(\"‚úÖ Multimodal model test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Understanding Multimodal Applications\n",
    "\n",
    "Multimodal AI enables exciting applications:\n",
    "\n",
    "1. **Image Captioning**: Generate text descriptions of images\n",
    "2. **Visual Question Answering**: Answer questions about image content\n",
    "3. **Image-Text Retrieval**: Find relevant images for text queries\n",
    "4. **Multimodal Chatbots**: AI assistants that understand both text and images\n",
    "5. **Content Generation**: Create images from text descriptions (like DALL-E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple image captioning model architecture\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    \"\"\"Simple image captioning model for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=512, max_seq_len=50):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Vision encoder\n",
    "        self.vision_encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 256, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((7, 7)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 7 * 7, d_model)\n",
    "        )\n",
    "        \n",
    "        # Caption decoder\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.decoder = nn.LSTM(d_model, d_model, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(d_model * 2, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, image, caption_tokens=None):\n",
    "        # Encode image\n",
    "        image_features = self.vision_encoder(image)  # [batch, d_model]\n",
    "        \n",
    "        if caption_tokens is not None:\n",
    "            # Training mode\n",
    "            caption_embeds = self.embedding(caption_tokens)\n",
    "            \n",
    "            # Initialize decoder with image features\n",
    "            batch_size = image.size(0)\n",
    "            h0 = image_features.unsqueeze(0).repeat(2, 1, 1)  # [num_layers, batch, d_model]\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            \n",
    "            # Decode captions\n",
    "            decoder_output, _ = self.decoder(caption_embeds, (h0, c0))\n",
    "            \n",
    "            # Project to vocabulary\n",
    "            logits = self.output_projection(decoder_output)\n",
    "            return logits\n",
    "        else:\n",
    "            # Inference mode (simplified)\n",
    "            return image_features\n",
    "\n",
    "# Create captioning model\n",
    "captioning_model = ImageCaptioningModel(vocab_size=5000)\n",
    "captioning_params = sum(p.numel() for p in captioning_model.parameters())\n",
    "\n",
    "print(f\"üì∏ Image Captioning Model Created!\")\n",
    "print(f\"Total parameters: {captioning_params:,}\")\n",
    "\n",
    "# Test the model\n",
    "test_image = torch.randn(1, 3, 224, 224)\n",
    "test_caption = torch.randint(0, 5000, (1, 20))\n",
    "\n",
    "caption_logits = captioning_model(test_image, test_caption)\n",
    "print(f\"Caption logits shape: {caption_logits.shape}\")\n",
    "print(\"‚úÖ Image captioning model test successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of multimodal training concepts\n",
    "print(\"üéì Multimodal Training Concepts\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulated multimodal training data\n",
    "def create_multimodal_training_example():\n",
    "    \"\"\"Create a sample multimodal training example\"\"\"\n",
    "    # Simulated image-caption pairs\n",
    "    examples = [\n",
    "        {\n",
    "            \"image_description\": \"A cat sitting on a windowsill\",\n",
    "            \"caption\": \"a fluffy orange cat sitting by the window\",\n",
    "            \"question\": \"What color is the cat?\",\n",
    "            \"answer\": \"orange\"\n",
    "        },\n",
    "        {\n",
    "            \"image_description\": \"A dog playing in a park\", \n",
    "            \"caption\": \"a happy golden retriever playing fetch\",\n",
    "            \"question\": \"What is the dog doing?\",\n",
    "            \"answer\": \"playing fetch\"\n",
    "        },\n",
    "        {\n",
    "            \"image_description\": \"A sunset over mountains\",\n",
    "            \"caption\": \"beautiful mountain landscape at sunset\",\n",
    "            \"question\": \"What time of day is shown?\",\n",
    "            \"answer\": \"sunset\"\n",
    "        }\n",
    "    ]\n",
    "    return examples\n",
    "\n",
    "# Show training examples\n",
    "training_examples = create_multimodal_training_example()\n",
    "print(\"Sample Multimodal Training Data:\")\n",
    "for i, example in enumerate(training_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Image: {example['image_description']}\")\n",
    "    print(f\"  Caption: {example['caption']}\")\n",
    "    print(f\"  VQA Question: {example['question']}\")\n",
    "    print(f\"  VQA Answer: {example['answer']}\")\n",
    "\n",
    "print(\"\\nüîß Key Multimodal Training Techniques:\")\n",
    "print(\"1. **Contrastive Learning**: Learning to match images with correct captions\")\n",
    "print(\"2. **Cross-Modal Attention**: Allowing text and images to attend to each other\")\n",
    "print(\"3. **Multi-Task Learning**: Training on multiple tasks simultaneously\")\n",
    "print(\"4. **Data Augmentation**: Augmenting both images and text for robustness\")\n",
    "print(\"5. **Progressive Training**: Starting with simple tasks, moving to complex ones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Advanced Multimodal Concepts\n",
    "\n",
    "**Current State-of-the-Art Models:**\n",
    "- **GPT-4V**: ChatGPT with vision capabilities\n",
    "- **DALL-E 2/3**: Text-to-image generation\n",
    "- **CLIP**: Connecting text and images\n",
    "- **LLaVA**: Large Language and Vision Assistant\n",
    "- **Flamingo**: Few-shot learning on multimodal tasks\n",
    "\n",
    "**Key Challenges:**\n",
    "1. **Alignment**: Ensuring different modalities are properly aligned\n",
    "2. **Scale**: Training large multimodal models requires significant compute\n",
    "3. **Data Quality**: High-quality paired multimodal data is often scarce\n",
    "4. **Evaluation**: Developing comprehensive evaluation metrics\n",
    "\n",
    "**Future Directions:**\n",
    "- Video understanding and generation\n",
    "- 3D scene understanding\n",
    "- Real-time multimodal interactions\n",
    "- Multimodal reasoning and planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics for multimodal models\n",
    "def demonstrate_multimodal_evaluation():\n",
    "    \"\"\"Show how to evaluate multimodal models\"\"\"\n",
    "    print(\"üìä Multimodal Evaluation Metrics\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Simulated image captioning evaluation\n",
    "    reference_captions = [\n",
    "        \"a brown dog sitting on green grass\",\n",
    "        \"two children playing in a park\",\n",
    "        \"a red car parked on the street\"\n",
    "    ]\n",
    "    \n",
    "    generated_captions = [\n",
    "        \"a dog sitting on grass\",\n",
    "        \"children playing outside\", \n",
    "        \"a red vehicle on the road\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìù Image Captioning Evaluation:\")\n",
    "    for i, (ref, gen) in enumerate(zip(reference_captions, generated_captions)):\n",
    "        # Simple word overlap calculation (simplified BLEU)\n",
    "        ref_words = set(ref.split())\n",
    "        gen_words = set(gen.split())\n",
    "        overlap = len(ref_words.intersection(gen_words))\n",
    "        total_words = len(gen_words)\n",
    "        precision = overlap / total_words if total_words > 0 else 0\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  Reference: '{ref}'\")\n",
    "        print(f\"  Generated: '{gen}'\")\n",
    "        print(f\"  Word Overlap Score: {precision:.2f}\")\n",
    "    \n",
    "    # VQA accuracy simulation\n",
    "    print(\"\\n‚ùì Visual Question Answering Evaluation:\")\n",
    "    vqa_results = [\n",
    "        {\"question\": \"What color is the car?\", \"predicted\": \"red\", \"actual\": \"red\", \"correct\": True},\n",
    "        {\"question\": \"How many people?\", \"predicted\": \"two\", \"actual\": \"three\", \"correct\": False},\n",
    "        {\"question\": \"What animal is shown?\", \"predicted\": \"dog\", \"actual\": \"dog\", \"correct\": True}\n",
    "    ]\n",
    "    \n",
    "    correct_answers = sum(1 for result in vqa_results if result[\"correct\"])\n",
    "    total_questions = len(vqa_results)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    \n",
    "    for result in vqa_results:\n",
    "        status = \"‚úÖ\" if result[\"correct\"] else \"‚ùå\"\n",
    "        print(f\"  {status} Q: {result['question']}\")\n",
    "        print(f\"     Predicted: {result['predicted']}, Actual: {result['actual']}\")\n",
    "    \n",
    "    print(f\"\\nüìà Overall VQA Accuracy: {accuracy:.1%}\")\n",
    "    \n",
    "    print(\"\\nüéØ Key Multimodal Metrics:\")\n",
    "    print(\"  ‚Ä¢ BLEU/ROUGE: For caption generation quality\")\n",
    "    print(\"  ‚Ä¢ CIDEr: Consensus-based caption evaluation\")\n",
    "    print(\"  ‚Ä¢ Accuracy: For classification tasks like VQA\")\n",
    "    print(\"  ‚Ä¢ Recall@K: For retrieval tasks\")\n",
    "    print(\"  ‚Ä¢ Human Evaluation: For quality and relevance\")\n",
    "\n",
    "demonstrate_multimodal_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåü Congratulations on Multimodal AI!\n",
    "\n",
    "You've now been introduced to the exciting world of **Multimodal AI**! This represents the cutting edge of artificial intelligence, where models can understand and generate content across multiple modalities.\n",
    "\n",
    "**What you've learned about multimodal AI:**\n",
    "- How to combine vision and language understanding\n",
    "- Architecture patterns for multimodal models\n",
    "- Applications like image captioning and VQA\n",
    "- Evaluation metrics for multimodal systems\n",
    "\n",
    "**Next steps in your multimodal AI journey:**\n",
    "1. Experiment with pre-trained multimodal models (CLIP, BLIP, etc.)\n",
    "2. Try building image captioning systems\n",
    "3. Explore visual question answering\n",
    "4. Study the latest research in multimodal AI\n",
    "5. Consider the ethical implications of multimodal systems\n",
    "\n",
    "The future of AI is multimodal - combining text, images, audio, and even video to create more intelligent and capable systems! üöÄüé®üîä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Advanced Techniques: RLHF and Model Alignment\n",
    "\n",
    "Now let's explore modern alignment techniques that make language models safer and more helpful!"
   ]
  },
  {
   "cell_type": "markdown", 
   "metadata": {},
   "source": [
    "### Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "RLHF is the key technique behind ChatGPT's conversational abilities. It consists of three stages:\n",
    "\n",
    "1. **Supervised Fine-Tuning (SFT)**: Train on high-quality instruction-following data\n",
    "2. **Reward Model Training**: Learn human preferences from comparison data\n",
    "3. **PPO Training**: Use reinforcement learning to optimize for human preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic preference data for RLHF demonstration\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain machine learning\",\n",
    "        \"chosen\": \"Machine learning is a subset of AI that enables computers to learn from data without explicit programming.\",\n",
    "        \"rejected\": \"Machine learning is when computers become smart and can think like humans.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How to stay healthy?\",\n",
    "        \"chosen\": \"Maintain a balanced diet, exercise regularly, get adequate sleep, and have regular health checkups.\",\n",
    "        \"rejected\": \"Just eat whatever you want and don't worry about it.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is Python?\",\n",
    "        \"chosen\": \"Python is a high-level programming language known for its simplicity and versatility, widely used in data science and AI.\",\n",
    "        \"rejected\": \"Python is a snake that programmers worship for some reason.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üéØ Preference Dataset for RLHF\")\n",
    "print(\"=\" * 40)\n",
    "for i, example in enumerate(preference_data):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Prompt: {example['prompt']}\")\n",
    "    print(f\"  ‚úÖ Chosen: {example['chosen']}\")\n",
    "    print(f\"  ‚ùå Rejected: {example['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Model Training\n",
    "\n",
    "The reward model learns to score responses based on human preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class SimpleRewardModel(nn.Module):\n",
    "    \"\"\"Simple reward model for RLHF demonstrations\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead=4, batch_first=True),\n",
    "            num_layers=2\n",
    "        )\n",
    "        self.reward_head = nn.Linear(d_model, 1)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(1)  # Mean pooling\n",
    "        reward = self.reward_head(x)\n",
    "        return reward.squeeze(-1)\n",
    "\n",
    "# Create a simple reward model\n",
    "reward_model = SimpleRewardModel(vocab_size=1000)\n",
    "optimizer = optim.Adam(reward_model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"üèÜ Reward Model Architecture:\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in reward_model.parameters()):,}\")\n",
    "print(\"  Input: Tokenized text\")\n",
    "print(\"  Output: Scalar reward score\")\n",
    "print(\"  Training: Bradley-Terry preference model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Training Simulation\n",
    "\n",
    "PPO (Proximal Policy Optimization) is used to fine-tune the language model using rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ppo_training():\n",
    "    \"\"\"Simulate PPO training process\"\"\"\n",
    "    print(\"üöÄ PPO Training Simulation\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Simulate training steps\n",
    "    prompts = [\n",
    "        \"Explain artificial intelligence\",\n",
    "        \"How to learn programming\", \n",
    "        \"What is machine learning\"\n",
    "    ]\n",
    "    \n",
    "    for step in range(3):\n",
    "        print(f\"\\nüìà Step {step + 1}:\")\n",
    "        \n",
    "        total_reward = 0\n",
    "        for prompt in prompts:\n",
    "            # Simulate reward calculation\n",
    "            helpful_response_reward = 0.8 + step * 0.1  # Improving over time\n",
    "            total_reward += helpful_response_reward\n",
    "            \n",
    "        avg_reward = total_reward / len(prompts)\n",
    "        print(f\"  Average Reward: {avg_reward:.2f}\")\n",
    "        print(f\"  Policy Update: Applied gradient with clipping\")\n",
    "        print(f\"  KL Penalty: {0.02 - step * 0.005:.3f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ PPO Key Components:\")\n",
    "    print(\"  ‚Ä¢ Importance Sampling: ratio = œÄ_new(a|s) / œÄ_old(a|s)\")\n",
    "    print(\"  ‚Ä¢ Clipped Objective: min(ratio √ó advantage, clip(ratio) √ó advantage)\")\n",
    "    print(\"  ‚Ä¢ KL Penalty: Œ≤ √ó KL(œÄ_new || œÄ_old)\")\n",
    "    print(\"  ‚Ä¢ Value Function: V(s) for advantage estimation\")\n",
    "\n",
    "simulate_ppo_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization (DPO)\n",
    "\n",
    "DPO is a simpler alternative to RLHF that directly optimizes preferences without reward modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_dpo():\n",
    "    \"\"\"Demonstrate DPO loss calculation\"\"\"\n",
    "    print(\"üéØ Direct Preference Optimization (DPO)\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Simulate log probabilities\n",
    "    examples = [\n",
    "        {\n",
    "            \"prompt\": \"Explain AI\",\n",
    "            \"chosen_logp_policy\": -2.1,\n",
    "            \"rejected_logp_policy\": -3.2,\n",
    "            \"chosen_logp_ref\": -2.5,\n",
    "            \"rejected_logp_ref\": -3.0\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    beta = 0.1  # Temperature parameter\n",
    "    \n",
    "    for i, ex in enumerate(examples):\n",
    "        policy_diff = ex[\"chosen_logp_policy\"] - ex[\"rejected_logp_policy\"]\n",
    "        ref_diff = ex[\"chosen_logp_ref\"] - ex[\"rejected_logp_ref\"]\n",
    "        \n",
    "        # DPO loss calculation\n",
    "        dpo_loss = -math.log(1 / (1 + math.exp(-beta * (policy_diff - ref_diff))))\n",
    "        \n",
    "        print(f\"\\nExample {i+1}: {ex['prompt']}\")\n",
    "        print(f\"  Policy preference: {policy_diff:.2f}\")\n",
    "        print(f\"  Reference preference: {ref_diff:.2f}\")\n",
    "        print(f\"  DPO Loss: {dpo_loss:.3f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ DPO Advantages:\")\n",
    "    print(\"  ‚Ä¢ No reward model needed\")\n",
    "    print(\"  ‚Ä¢ More stable training\")\n",
    "    print(\"  ‚Ä¢ Direct preference optimization\")\n",
    "    print(\"  ‚Ä¢ Simpler implementation\")\n",
    "\n",
    "demonstrate_dpo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constitutional AI\n",
    "\n",
    "Constitutional AI trains models to follow explicit principles or \"constitution\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constitutional_ai_demo():\n",
    "    \"\"\"Demonstrate Constitutional AI principles\"\"\"\n",
    "    print(\"üìú Constitutional AI Demo\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Define AI constitution\n",
    "    constitution = [\n",
    "        \"Be helpful, harmless, and honest\",\n",
    "        \"Respect human autonomy and dignity\", \n",
    "        \"Provide accurate information\",\n",
    "        \"Admit uncertainty when appropriate\",\n",
    "        \"Avoid generating harmful content\"\n",
    "    ]\n",
    "    \n",
    "    print(\"üèõÔ∏è  AI Constitution:\")\n",
    "    for i, principle in enumerate(constitution, 1):\n",
    "        print(f\"  {i}. {principle}\")\n",
    "    \n",
    "    # Example scenarios\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"user_input\": \"How to hack into someone's computer?\",\n",
    "            \"initial_response\": \"I can't provide instructions for unauthorized access.\",\n",
    "            \"critique\": \"‚úÖ Good - follows principle of avoiding harmful content.\",\n",
    "            \"revision\": \"N/A - Already constitutional.\"\n",
    "        },\n",
    "        {\n",
    "            \"user_input\": \"What's the capital of Mars?\",\n",
    "            \"initial_response\": \"The capital of Mars is New Geneva.\",\n",
    "            \"critique\": \"‚ùå Provides false information. Should admit uncertainty.\",\n",
    "            \"revision\": \"Mars doesn't have a capital as it lacks human settlements.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîç Constitutional AI in Action:\")\n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        print(f\"\\nScenario {i}:\")\n",
    "        print(f\"  Input: {scenario['user_input']}\")\n",
    "        print(f\"  Initial: {scenario['initial_response']}\")\n",
    "        print(f\"  Critique: {scenario['critique']}\")\n",
    "        print(f\"  Revision: {scenario['revision']}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Constitutional Process:\")\n",
    "    print(\"  1. Generate initial response\")\n",
    "    print(\"  2. Critique against constitution\")\n",
    "    print(\"  3. Revise if needed\")\n",
    "    print(\"  4. Repeat until compliance\")\n",
    "\n",
    "constitutional_ai_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Reasoning\n",
    "\n",
    "CoT prompting improves model reasoning by encouraging step-by-step thinking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_demo():\n",
    "    \"\"\"Demonstrate Chain-of-Thought reasoning\"\"\"\n",
    "    print(\"üß† Chain-of-Thought (CoT) Reasoning\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    problem = \"If a train travels 240 miles in 4 hours, what is its average speed?\"\n",
    "    \n",
    "    print(f\"Problem: {problem}\")\n",
    "    \n",
    "    # Standard vs CoT comparison\n",
    "    print(\"\\n‚ùå Standard Response:\")\n",
    "    print(\"The speed is 60 mph.\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Chain-of-Thought Response:\")\n",
    "    print(\"Let me think step by step:\")\n",
    "    print(\"1. I need to find average speed\")\n",
    "    print(\"2. Speed = Distance / Time\")\n",
    "    print(\"3. Distance = 240 miles\")\n",
    "    print(\"4. Time = 4 hours\")\n",
    "    print(\"5. Speed = 240 √∑ 4 = 60 mph\")\n",
    "    print(\"Therefore, the average speed is 60 mph.\")\n",
    "    \n",
    "    print(\"\\nüéØ CoT Benefits:\")\n",
    "    print(\"  ‚Ä¢ Improved reasoning quality\")\n",
    "    print(\"  ‚Ä¢ Better problem decomposition\")\n",
    "    print(\"  ‚Ä¢ More transparent thinking\")\n",
    "    print(\"  ‚Ä¢ Reduced errors in complex problems\")\n",
    "    \n",
    "    # Few-shot examples\n",
    "    print(\"\\nüìö Few-Shot CoT Examples:\")\n",
    "    examples = [\n",
    "        \"Q: 15 + 27 = ?\\nA: 15 + 27 = 15 + 20 + 7 = 35 + 7 = 42\",\n",
    "        \"Q: 8 √ó 9 = ?\\nA: 8 √ó 9 = 8 √ó 10 - 8 √ó 1 = 80 - 8 = 72\"\n",
    "    ]\n",
    "    \n",
    "    for example in examples:\n",
    "        print(f\"  {example}\")\n",
    "\n",
    "chain_of_thought_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Alignment Summary\n",
    "\n",
    "Let's compare the different alignment techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create comparison table\n",
    "alignment_techniques = {\n",
    "    'Technique': ['RLHF', 'DPO', 'Constitutional AI', 'RLAIF', 'Chain-of-Thought'],\n",
    "    'Complexity': ['High', 'Medium', 'Medium', 'High', 'Low'],\n",
    "    'Data Required': ['Preferences', 'Preferences', 'Principles', 'AI-Generated', 'Examples'],\n",
    "    'Key Benefit': ['Human Alignment', 'Stability', 'Transparency', 'Scalability', 'Reasoning'],\n",
    "    'Use Case': ['General Chat', 'Preference Tasks', 'Safety-Critical', 'Large Scale', 'Math/Logic']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(alignment_techniques)\n",
    "\n",
    "print(\"üéØ Model Alignment Techniques Comparison\")\n",
    "print(\"=\" * 45)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüåü Key Takeaways:\")\n",
    "print(\"  ‚Ä¢ RLHF: Gold standard for conversational AI\")\n",
    "print(\"  ‚Ä¢ DPO: Simpler alternative to RLHF\")\n",
    "print(\"  ‚Ä¢ Constitutional AI: Explicit principle-based training\")\n",
    "print(\"  ‚Ä¢ RLAIF: Use AI feedback instead of human feedback\")\n",
    "print(\"  ‚Ä¢ CoT: Enhance reasoning through structured prompting\")\n",
    "\n",
    "print(\"\\nüìà Research Frontiers:\")\n",
    "print(\"  ‚Ä¢ Scalable oversight methods\")\n",
    "print(\"  ‚Ä¢ Interpretability and transparency\")\n",
    "print(\"  ‚Ä¢ Robustness to distribution shift\")\n",
    "print(\"  ‚Ä¢ Multi-agent alignment scenarios\")\n",
    "print(\"  ‚Ä¢ Value learning and specification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built and trained your own Large Language Model, explored Multimodal AI, **and** learned about modern alignment techniques! Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ What You've Learned:\n",
    "1. **Transformer Architecture** - Built multi-head attention and transformer blocks from scratch\n",
    "2. **Language Modeling** - Understood the mathematical foundations of language modeling\n",
    "3. **Tokenization** - Created a tokenizer to convert text to numbers\n",
    "4. **Training Process** - Implemented a complete training loop with proper optimization\n",
    "5. **Text Generation** - Used your model to generate new text\n",
    "6. **Model Evaluation** - Measured performance using perplexity\n",
    "7. **Model Persistence** - Saved and loaded your trained model\n",
    "8. **üÜï Multimodal AI** - Explored vision-language models and multimodal applications\n",
    "9. **üéØ RLHF & Alignment** - Learned modern techniques for model alignment and safety\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Scale Up**: Try training on larger datasets with more parameters\n",
    "2. **Pre-trained Models**: Experiment with fine-tuning GPT-2, GPT-4, or other models\n",
    "3. **Advanced Techniques**: Learn about PEFT methods like LoRA and QLoRA\n",
    "4. **üéØ RLHF Implementation**: Build real preference datasets and reward models\n",
    "5. **Specialized Domains**: Train models on specific domains (code, science, literature)\n",
    "6. **üé® Multimodal Projects**: Build image captioning, VQA, or text-to-image systems\n",
    "7. **üîä Audio Integration**: Explore speech recognition and text-to-speech models\n",
    "8. **üé¨ Video Understanding**: Investigate video analysis and generation\n",
    "\n",
    "### üí° Key Takeaways:\n",
    "- **Start Small**: Begin with small models and datasets to understand the concepts\n",
    "- **Quality Data**: The quality of your training data matters more than quantity\n",
    "- **Evaluation**: Always evaluate your models thoroughly before deployment\n",
    "- **Ethics & Safety**: Consider ethical implications and use alignment techniques\n",
    "- **Human Feedback**: RLHF and similar methods are crucial for helpful AI\n",
    "- **Multimodal Future**: The future of AI combines multiple modalities\n",
    "- **Alignment Matters**: Safe and beneficial AI requires careful alignment work\n",
    "- **Continuous Learning**: The field is rapidly evolving - keep learning!\n",
    "\n",
    "You now have the foundational knowledge to build and train text-only and multimodal AI systems with modern alignment techniques. The methods you've learned are used in state-of-the-art models like GPT-4, Claude, ChatGPT, and other cutting-edge AI systems!\n",
    "\n",
    "Happy building! ü§ñ‚ú®üé®üîäüé¨üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}