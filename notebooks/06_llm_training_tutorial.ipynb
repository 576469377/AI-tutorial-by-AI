{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Training Tutorial\n",
    "\n",
    "Welcome to the comprehensive tutorial on training Large Language Models! This notebook will guide you through:\n",
    "\n",
    "1. **Understanding Transformers** - The architecture behind modern LLMs\n",
    "2. **Building from Scratch** - Implementing a simple language model\n",
    "3. **Training Process** - Learning how to train your own model\n",
    "4. **Text Generation** - Using your model to generate text\n",
    "5. **Fine-tuning** - Adapting pre-trained models\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Transformer Architecture\n",
    "\n",
    "Let's start by implementing the core components of a transformer model:\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "The attention mechanism is the heart of transformers. It allows the model to focus on different parts of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads and project\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        return self.w_o(attention_output)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        return torch.matmul(attention_weights, V)\n",
    "\n",
    "# Test the attention mechanism\n",
    "attention = MultiHeadAttention(d_model=128, num_heads=8)\n",
    "x = torch.randn(2, 10, 128)  # batch_size=2, seq_len=10, d_model=128\n",
    "output = attention(x, x, x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úÖ Multi-head attention working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "Now let's build a complete transformer block that combines attention with a feed-forward network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with self-attention and feed-forward network\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attended = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attended))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        fed_forward = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(fed_forward))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test the transformer block\n",
    "transformer_block = TransformerBlock(d_model=128, num_heads=8, d_ff=512)\n",
    "x = torch.randn(2, 10, 128)\n",
    "output = transformer_block(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úÖ Transformer block working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Simple Language Model\n",
    "\n",
    "Now let's create a complete language model using our transformer components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create a causal mask to prevent attention to future positions\"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    return mask == 0  # True for allowed positions, False for masked\n",
    "\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    \"\"\"A simple transformer-based language model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=4, \n",
    "                 d_ff=1024, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        \n",
    "        # Transformer layers\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layers\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create position indices\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Embeddings\n",
    "        token_embeds = self.token_embedding(input_ids)\n",
    "        pos_embeds = self.position_embedding(positions)\n",
    "        x = token_embeds + pos_embeds\n",
    "        \n",
    "        # Create causal mask\n",
    "        causal_mask = create_causal_mask(seq_len).to(input_ids.device)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, causal_mask)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create a model\n",
    "vocab_size = 1000  # We'll build a proper vocabulary later\n",
    "model = SimpleLanguageModel(vocab_size=vocab_size, d_model=256, num_heads=8, num_layers=4)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model created with {total_params:,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, vocab_size, (2, 20))  # batch_size=2, seq_len=20\n",
    "output = model(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"‚úÖ Language model working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization and Data Preparation\n",
    "\n",
    "Before we can train our model, we need to convert text into tokens that the model can understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"Simple word-level tokenizer for demonstration purposes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        self.bos_token = '<BOS>'\n",
    "        \n",
    "        # Add special tokens\n",
    "        self._add_word(self.pad_token)\n",
    "        self._add_word(self.unk_token)\n",
    "        self._add_word(self.eos_token)\n",
    "        self._add_word(self.bos_token)\n",
    "        \n",
    "        self.pad_token_id = self.word_to_id[self.pad_token]\n",
    "        self.unk_token_id = self.word_to_id[self.unk_token]\n",
    "        self.eos_token_id = self.word_to_id[self.eos_token]\n",
    "        self.bos_token_id = self.word_to_id[self.bos_token]\n",
    "    \n",
    "    def _add_word(self, word):\n",
    "        if word not in self.word_to_id:\n",
    "            self.word_to_id[word] = self.vocab_size\n",
    "            self.id_to_word[self.vocab_size] = word\n",
    "            self.vocab_size += 1\n",
    "        return self.word_to_id[word]\n",
    "    \n",
    "    def build_vocab(self, texts, min_freq=1):\n",
    "        \"\"\"Build vocabulary from list of texts\"\"\"\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Add words that meet minimum frequency threshold\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= min_freq:\n",
    "                self._add_word(word)\n",
    "        \n",
    "        print(f\"Built vocabulary with {self.vocab_size} tokens\")\n",
    "    \n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"Convert text to list of token IDs\"\"\"\n",
    "        words = text.lower().split()\n",
    "        token_ids = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            token_ids.append(self.bos_token_id)\n",
    "        \n",
    "        for word in words:\n",
    "            token_id = self.word_to_id.get(word, self.unk_token_id)\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            token_ids.append(self.eos_token_id)\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        \"\"\"Convert list of token IDs back to text\"\"\"\n",
    "        words = []\n",
    "        for token_id in token_ids:\n",
    "            word = self.id_to_word.get(token_id, self.unk_token)\n",
    "            if skip_special_tokens and word in [self.pad_token, self.unk_token, self.eos_token, self.bos_token]:\n",
    "                continue\n",
    "            words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Create sample text data\n",
    "sample_texts = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"machine learning is a subset of artificial intelligence\",\n",
    "    \"neural networks are inspired by biological neural networks\", \n",
    "    \"deep learning uses multiple layers to learn representations\",\n",
    "    \"transformers use attention mechanisms for better performance\",\n",
    "    \"language models predict the next word in a sequence\",\n",
    "    \"artificial intelligence will transform many industries\",\n",
    "    \"data science combines statistics programming and domain knowledge\",\n",
    "    \"python is a popular programming language for machine learning\",\n",
    "    \"the future of technology depends on continued innovation\"\n",
    "]\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(sample_texts)\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"machine learning is fascinating\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded text: {decoded}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and Training Setup\n",
    "\n",
    "Let's create a dataset class and set up the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for language modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, tokenizer, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        \n",
    "        for text in texts:\n",
    "            token_ids = tokenizer.encode(text)\n",
    "            \n",
    "            # Split long texts into chunks\n",
    "            for i in range(0, len(token_ids) - max_length + 1, max_length // 2):\n",
    "                chunk = token_ids[i:i + max_length]\n",
    "                if len(chunk) == max_length:\n",
    "                    self.examples.append(chunk)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.examples[idx], dtype=torch.long)\n",
    "\n",
    "# Create expanded dataset for better training\n",
    "expanded_texts = sample_texts * 50  # Repeat texts to have more training data\n",
    "\n",
    "# Split into train and validation\n",
    "split_idx = int(0.8 * len(expanded_texts))\n",
    "train_texts = expanded_texts[:split_idx]\n",
    "val_texts = expanded_texts[split_idx:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_texts, tokenizer, max_length=32)\n",
    "val_dataset = TextDataset(val_texts, tokenizer, max_length=32)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Show a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample text: {tokenizer.decode(sample_batch[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Language Model\n",
    "\n",
    "Now let's train our language model! We'll track the loss and visualize the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with the correct vocabulary size\n",
    "model = SimpleLanguageModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_layers=3,\n",
    "    d_ff=512,\n",
    "    max_seq_len=64\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model has {total_params:,} parameters\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        # For language modeling, targets are input shifted by one position\n",
    "        inputs = batch[:, :-1]\n",
    "        targets = batch[:, 1:]\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            epoch_val_loss += loss.item()\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_train_loss = epoch_train_loss / num_batches\n",
    "    avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Training Progress\n",
    "\n",
    "Let's plot the training and validation losses to see how our model learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Language Model Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate perplexity (lower is better)\n",
    "final_train_perplexity = math.exp(train_losses[-1])\n",
    "final_val_perplexity = math.exp(val_losses[-1])\n",
    "\n",
    "print(f\"Final Training Perplexity: {final_train_perplexity:.2f}\")\n",
    "print(f\"Final Validation Perplexity: {final_val_perplexity:.2f}\")\n",
    "\n",
    "# Show training statistics\n",
    "print(f\"\\nTraining Statistics:\")\n",
    "print(f\"Initial Train Loss: {train_losses[0]:.4f}\")\n",
    "print(f\"Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Loss Reduction: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Generation\n",
    "\n",
    "Now for the exciting part - let's use our trained model to generate text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_k=10):\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt, add_special_tokens=False), \n",
    "                           dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    generated_tokens = input_ids.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get model predictions\n",
    "            logits = model(generated_tokens)\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "                next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n",
    "                next_token_logits[top_k_indices] = top_k_logits\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probabilities, 1)\n",
    "            \n",
    "            # Stop if we generate an end-of-sequence token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    # Decode and return generated text\n",
    "    generated_text = tokenizer.decode(generated_tokens[0].tolist(), skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test text generation with different prompts\n",
    "test_prompts = [\n",
    "    \"machine learning\",\n",
    "    \"the future of\",\n",
    "    \"artificial intelligence\",\n",
    "    \"neural networks\",\n",
    "    \"deep learning\"\n",
    "]\n",
    "\n",
    "print(\"ü§ñ Text Generation Examples\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=15, temperature=0.8, top_k=10)\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Generated: '{generated}'\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Try with different temperatures\n",
    "print(\"\\nüå°Ô∏è Temperature Effects on Generation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "prompt = \"artificial intelligence\"\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated = generate_text(model, tokenizer, prompt, max_length=15, temperature=temp, top_k=10)\n",
    "    print(f\"Temperature {temp}: '{generated}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance using perplexity and other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_detailed_perplexity(model, data_loader):\n",
    "    \"\"\"Calculate perplexity with detailed statistics\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss, total_tokens\n",
    "\n",
    "# Calculate perplexity for train and validation sets\n",
    "train_perplexity, train_loss, train_tokens = calculate_detailed_perplexity(model, train_loader)\n",
    "val_perplexity, val_loss, val_tokens = calculate_detailed_perplexity(model, val_loader)\n",
    "\n",
    "print(\"üìä Model Evaluation Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  Perplexity: {train_perplexity:.2f}\")\n",
    "print(f\"  Loss: {train_loss:.4f}\")\n",
    "print(f\"  Tokens evaluated: {train_tokens:,}\")\n",
    "print()\n",
    "print(f\"Validation Set:\")\n",
    "print(f\"  Perplexity: {val_perplexity:.2f}\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  Tokens evaluated: {val_tokens:,}\")\n",
    "print()\n",
    "\n",
    "# Model size and efficiency metrics\n",
    "model_size_mb = total_params * 4 / (1024 * 1024)  # Assuming float32\n",
    "print(f\"Model Statistics:\")\n",
    "print(f\"  Parameters: {total_params:,}\")\n",
    "print(f\"  Model size: {model_size_mb:.1f} MB\")\n",
    "print(f\"  Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Max sequence length: {model.max_seq_len}\")\n",
    "\n",
    "# Analyze training efficiency\n",
    "improvement = (train_losses[0] - train_losses[-1]) / train_losses[0] * 100\n",
    "print(f\"\\nTraining Efficiency:\")\n",
    "print(f\"  Initial loss: {train_losses[0]:.4f}\")\n",
    "print(f\"  Final loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.1f}%\")\n",
    "print(f\"  Epochs trained: {len(train_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Fine-tuning with Transformers Library\n",
    "\n",
    "Now let's see how to fine-tune a pre-trained model using the Transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell demonstrates fine-tuning with transformers library\n",
    "# Uncomment and run if you have transformers installed\n",
    "\n",
    "# try:\n",
    "#     from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer\n",
    "#     \n",
    "#     print(\"ü§ó Fine-tuning with Transformers Library\")\n",
    "#     print(\"=\" * 50)\n",
    "#     \n",
    "#     # Load a pre-trained model (small for demo)\n",
    "#     model_name = \"gpt2\"\n",
    "#     pretrained_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#     pretrained_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     \n",
    "#     # Add padding token\n",
    "#     if pretrained_tokenizer.pad_token is None:\n",
    "#         pretrained_tokenizer.pad_token = pretrained_tokenizer.eos_token\n",
    "#     \n",
    "#     print(f\"Loaded pre-trained model: {model_name}\")\n",
    "#     print(f\"Model parameters: {sum(p.numel() for p in pretrained_model.parameters()):,}\")\n",
    "#     print(f\"Vocabulary size: {len(pretrained_tokenizer)}\")\n",
    "#     \n",
    "#     # Generate some text with the pre-trained model\n",
    "#     prompt = \"Artificial intelligence is\"\n",
    "#     inputs = pretrained_tokenizer.encode(prompt, return_tensors='pt')\n",
    "#     \n",
    "#     with torch.no_grad():\n",
    "#         outputs = pretrained_model.generate(\n",
    "#             inputs, \n",
    "#             max_length=50, \n",
    "#             temperature=0.8, \n",
    "#             do_sample=True,\n",
    "#             pad_token_id=pretrained_tokenizer.eos_token_id\n",
    "#         )\n",
    "#     \n",
    "#     generated_text = pretrained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     print(f\"\\nPre-trained model generation:\")\n",
    "#     print(f\"Prompt: '{prompt}'\")\n",
    "#     print(f\"Generated: '{generated_text}'\")\n",
    "#     \n",
    "# except ImportError:\n",
    "#     print(\"Transformers library not available.\")\n",
    "#     print(\"Install with: pip install transformers\")\n",
    "\n",
    "print(\"üí° Fine-tuning Tips:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"1. Start with a pre-trained model for better results\")\n",
    "print(\"2. Use smaller learning rates for fine-tuning (1e-5 to 5e-5)\")\n",
    "print(\"3. Fine-tune for fewer epochs to avoid overfitting\")\n",
    "print(\"4. Use domain-specific data for better task performance\")\n",
    "print(\"5. Consider parameter-efficient methods like LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Load Your Model\n",
    "\n",
    "Let's save our trained model so we can use it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model_save_path = 'my_language_model.pth'\n",
    "tokenizer_save_path = 'my_tokenizer.json'\n",
    "\n",
    "# Save model state\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'd_model': 128,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 3,\n",
    "        'd_ff': 512,\n",
    "        'max_seq_len': 64\n",
    "    },\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'final_perplexity': val_perplexity\n",
    "}, model_save_path)\n",
    "\n",
    "# Save tokenizer\n",
    "import json\n",
    "tokenizer_data = {\n",
    "    'word_to_id': tokenizer.word_to_id,\n",
    "    'id_to_word': tokenizer.id_to_word,\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'special_tokens': {\n",
    "        'pad_token': tokenizer.pad_token,\n",
    "        'unk_token': tokenizer.unk_token,\n",
    "        'eos_token': tokenizer.eos_token,\n",
    "        'bos_token': tokenizer.bos_token\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(tokenizer_save_path, 'w') as f:\n",
    "    json.dump(tokenizer_data, f)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_save_path}\")\n",
    "print(f\"‚úÖ Tokenizer saved to: {tokenizer_save_path}\")\n",
    "\n",
    "# Demonstrate loading the model\n",
    "def load_model_and_tokenizer(model_path, tokenizer_path):\n",
    "    \"\"\"Load a saved model and tokenizer\"\"\"\n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    config = checkpoint['model_config']\n",
    "    \n",
    "    # Create model with saved configuration\n",
    "    loaded_model = SimpleLanguageModel(**config)\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loaded_model.to(device)\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    with open(tokenizer_path, 'r') as f:\n",
    "        tokenizer_data = json.load(f)\n",
    "    \n",
    "    loaded_tokenizer = SimpleTokenizer()\n",
    "    loaded_tokenizer.word_to_id = tokenizer_data['word_to_id']\n",
    "    loaded_tokenizer.id_to_word = {int(k): v for k, v in tokenizer_data['id_to_word'].items()}\n",
    "    loaded_tokenizer.vocab_size = tokenizer_data['vocab_size']\n",
    "    \n",
    "    return loaded_model, loaded_tokenizer\n",
    "\n",
    "# Test loading\n",
    "print(\"\\nüîÑ Testing model loading...\")\n",
    "loaded_model, loaded_tokenizer = load_model_and_tokenizer(model_save_path, tokenizer_save_path)\n",
    "\n",
    "# Test generation with loaded model\n",
    "test_prompt = \"machine learning\"\n",
    "generated = generate_text(loaded_model, loaded_tokenizer, test_prompt, max_length=10)\n",
    "print(f\"Loaded model generation: '{generated}'\")\n",
    "print(\"‚úÖ Model loading successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built and trained your own Large Language Model! Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ What You've Learned:\n",
    "1. **Transformer Architecture** - Built multi-head attention and transformer blocks from scratch\n",
    "2. **Language Modeling** - Understood the mathematical foundations of language modeling\n",
    "3. **Tokenization** - Created a tokenizer to convert text to numbers\n",
    "4. **Training Process** - Implemented a complete training loop with proper optimization\n",
    "5. **Text Generation** - Used your model to generate new text\n",
    "6. **Model Evaluation** - Measured performance using perplexity\n",
    "7. **Model Persistence** - Saved and loaded your trained model\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Scale Up**: Try training on larger datasets with more parameters\n",
    "2. **Pre-trained Models**: Experiment with fine-tuning GPT-2, GPT-3.5, or other models\n",
    "3. **Advanced Techniques**: Learn about PEFT methods like LoRA and QLoRA\n",
    "4. **Specialized Domains**: Train models on specific domains (code, science, literature)\n",
    "5. **Multimodal Models**: Explore models that combine text with images or other modalities\n",
    "\n",
    "### üí° Key Takeaways:\n",
    "- **Start Small**: Begin with small models and datasets to understand the concepts\n",
    "- **Quality Data**: The quality of your training data matters more than quantity\n",
    "- **Evaluation**: Always evaluate your models thoroughly before deployment\n",
    "- **Ethics**: Consider the ethical implications of your language models\n",
    "- **Continuous Learning**: The field is rapidly evolving - keep learning!\n",
    "\n",
    "You now have the foundational knowledge to build and train your own Large Language Models. The techniques you've learned here are the same ones used to create state-of-the-art models like GPT-4, Claude, and others!\n",
    "\n",
    "Happy building! ü§ñ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}