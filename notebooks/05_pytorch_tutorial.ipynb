{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Deep Learning Tutorial\n",
    "\n",
    "This notebook provides a comprehensive introduction to PyTorch, covering everything from basic tensor operations to building and training neural networks.\n",
    "\n",
    "## What You'll Learn\n",
    "1. **PyTorch Fundamentals** - Tensors, operations, and autograd\n",
    "2. **Neural Networks** - Building models with nn.Module\n",
    "3. **Training Loops** - Complete training pipeline\n",
    "4. **CNNs** - Convolutional networks for image data\n",
    "5. **RNNs** - Recurrent networks for sequences\n",
    "6. **Advanced Topics** - Transfer learning, optimization\n",
    "\n",
    "Let's start by importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Fundamentals\n",
    "\n",
    "### Tensors: The Building Blocks\n",
    "\n",
    "Tensors are multi-dimensional arrays, similar to NumPy arrays but with additional capabilities for GPU computation and automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors\n",
    "print(\"=== Creating Tensors ===\")\n",
    "\n",
    "# From Python lists\n",
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"From list: {x}\")\n",
    "print(f\"Data type: {x.dtype}\")\n",
    "\n",
    "# Special tensors\n",
    "zeros = torch.zeros(3, 4)\n",
    "ones = torch.ones(2, 3)\n",
    "random = torch.randn(2, 3)  # Normal distribution\n",
    "uniform = torch.rand(2, 3)   # Uniform distribution [0, 1)\n",
    "\n",
    "print(f\"\\nZeros tensor shape {zeros.shape}:\")\n",
    "print(zeros)\n",
    "print(f\"\\nRandom tensor:\")\n",
    "print(random)\n",
    "\n",
    "# From NumPy arrays\n",
    "numpy_array = np.array([1, 2, 3, 4, 5])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"\\nFrom NumPy: {tensor_from_numpy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor operations\n",
    "print(\"=== Tensor Operations ===\")\n",
    "\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "\n",
    "# Element-wise operations\n",
    "print(f\"\\nElement-wise operations:\")\n",
    "print(f\"a + b = {a + b}\")\n",
    "print(f\"a * b = {a * b}\")\n",
    "print(f\"a ** 2 = {a ** 2}\")\n",
    "\n",
    "# Reduction operations\n",
    "print(f\"\\nReduction operations:\")\n",
    "print(f\"Sum: {torch.sum(a)}\")\n",
    "print(f\"Mean: {torch.mean(a)}\")\n",
    "print(f\"Dot product: {torch.dot(a, b)}\")\n",
    "\n",
    "# Matrix operations\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 2)\n",
    "C = torch.mm(A, B)  # Matrix multiplication\n",
    "\n",
    "print(f\"\\nMatrix multiplication:\")\n",
    "print(f\"A shape: {A.shape}, B shape: {B.shape}\")\n",
    "print(f\"C = A @ B, shape: {C.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation (Autograd)\n",
    "\n",
    "PyTorch's autograd system automatically computes gradients for backpropagation. This is the foundation of neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Automatic Differentiation ===\")\n",
    "\n",
    "# Enable gradient computation\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x.item()}, requires_grad = {x.requires_grad}\")\n",
    "\n",
    "# Define a function: y = xÂ² + 3x + 1\n",
    "y = x**2 + 3*x + 1\n",
    "print(f\"y = xÂ² + 3x + 1 = {y.item()}\")\n",
    "\n",
    "# Compute gradient dy/dx\n",
    "y.backward()\n",
    "print(f\"dy/dx = 2x + 3 = {x.grad.item()}\")\n",
    "print(f\"At x=2: dy/dx = 2(2) + 3 = {2*2 + 3}\")\n",
    "\n",
    "# Multiple variables\n",
    "print(\"\\n=== Multiple Variables ===\")\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([2.0], requires_grad=True)\n",
    "z = x**2 + y**3 + x*y\n",
    "\n",
    "print(f\"z = xÂ² + yÂ³ + xy = {z.item()}\")\n",
    "z.backward()\n",
    "print(f\"âˆ‚z/âˆ‚x = 2x + y = {x.grad.item()}\")\n",
    "print(f\"âˆ‚z/âˆ‚y = 3yÂ² + x = {y.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Neural Networks\n",
    "\n",
    "### Creating a Simple Neural Network\n",
    "\n",
    "All neural networks in PyTorch inherit from `nn.Module`. This provides the structure and functionality needed for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.fc1(x))  # First layer + ReLU activation\n",
    "        x = self.dropout(x)      # Dropout for regularization\n",
    "        x = F.relu(self.fc2(x))  # Second layer + ReLU activation\n",
    "        x = self.dropout(x)      # More dropout\n",
    "        x = self.fc3(x)          # Output layer (no activation)\n",
    "        return x\n",
    "\n",
    "# Create a model instance\n",
    "model = SimpleNN(input_size=10, hidden_size=64, output_size=3)\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "\n",
    "Let's test our model with some random input to make sure it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with random input\n",
    "test_input = torch.randn(5, 10)  # Batch of 5 samples, 10 features each\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "model.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    output = model(test_input)\n",
    "    \n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\")\n",
    "print(output)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = F.softmax(output, dim=1)\n",
    "print(f\"\\nProbabilities (after softmax):\")\n",
    "print(probabilities)\n",
    "print(f\"\\nSum of probabilities for each sample: {probabilities.sum(dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a Neural Network\n",
    "\n",
    "### Creating Training Data\n",
    "\n",
    "Let's create a classification dataset to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=8, \n",
    "                          n_redundant=2, n_classes=3, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Training Loop\n",
    "\n",
    "Now let's implement a complete training loop with loss computation, backpropagation, and optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleNN(input_size=10, hidden_size=64, output_size=3)\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Model: {model.__class__.__name__}\")\n",
    "print(f\"Loss function: {criterion.__class__.__name__}\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Training loop\n",
    "model.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients from previous iteration\n",
    "        loss.backward()        # Compute gradients\n",
    "        optimizer.step()       # Update parameters\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += batch_y.size(0)\n",
    "        correct_predictions += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 20 == 0 or epoch == num_epochs - 1:\n",
    "        print(f\"Epoch [{epoch:3d}/{num_epochs}] - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Let's evaluate our trained model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()  # Set to evaluation mode\n",
    "test_loss = 0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += batch_y.size(0)\n",
    "        correct_predictions += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(batch_y.cpu().numpy())\n",
    "\n",
    "test_accuracy = correct_predictions / total_samples\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "print(f\"Test Results:\")\n",
    "print(f\"Average Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} ({correct_predictions}/{total_samples})\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(all_targets, all_predictions)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Progress\n",
    "\n",
    "Let's plot the training curves to see how our model learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy curve\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accuracies)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# Confusion matrix\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(np.unique(all_targets)))\n",
    "plt.xticks(tick_marks, range(len(np.unique(all_targets))))\n",
    "plt.yticks(tick_marks, range(len(np.unique(all_targets))))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Add text annotations to confusion matrix\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training accuracy: {train_accuracies[-1]:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are specialized for processing grid-like data such as images. Let's build a simple CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple Convolutional Neural Network\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # Note: Input size calculation depends on image dimensions\n",
    "        # For 28x28 images: after two 2x2 pooling operations -> 7x7\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # 28x28 -> 14x14\n",
    "        # Second convolutional block\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # 14x14 -> 7x7\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = SimpleCNN(num_classes=10)\n",
    "print(\"CNN Architecture:\")\n",
    "print(cnn_model)\n",
    "\n",
    "# Test with synthetic image data\n",
    "batch_size = 4\n",
    "test_images = torch.randn(batch_size, 1, 28, 28)  # Grayscale 28x28 images\n",
    "print(f\"\\nInput shape: {test_images.shape}\")\n",
    "\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = cnn_model(test_images)\n",
    "    \n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output logits:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding CNN Layers\n",
    "\n",
    "Let's examine what each layer in our CNN does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CNN layer outputs\n",
    "test_image = torch.randn(1, 1, 28, 28)  # Single grayscale image\n",
    "print(f\"Input image shape: {test_image.shape}\")\n",
    "\n",
    "# Forward pass through each layer\n",
    "with torch.no_grad():\n",
    "    # First conv layer\n",
    "    conv1_out = F.relu(cnn_model.conv1(test_image))\n",
    "    print(f\"After conv1 + ReLU: {conv1_out.shape}\")\n",
    "    \n",
    "    # First pooling\n",
    "    pool1_out = cnn_model.pool(conv1_out)\n",
    "    print(f\"After pool1: {pool1_out.shape}\")\n",
    "    \n",
    "    # Second conv layer\n",
    "    conv2_out = F.relu(cnn_model.conv2(pool1_out))\n",
    "    print(f\"After conv2 + ReLU: {conv2_out.shape}\")\n",
    "    \n",
    "    # Second pooling\n",
    "    pool2_out = cnn_model.pool(conv2_out)\n",
    "    print(f\"After pool2: {pool2_out.shape}\")\n",
    "    \n",
    "    # Flatten\n",
    "    flattened = pool2_out.view(-1, 64 * 7 * 7)\n",
    "    print(f\"After flattening: {flattened.shape}\")\n",
    "\n",
    "# Visualize some filters from the first conv layer\n",
    "plt.figure(figsize=(12, 3))\n",
    "filters = cnn_model.conv1.weight.data\n",
    "print(f\"\\nFirst layer has {filters.shape[0]} filters of size {filters.shape[2]}x{filters.shape[3]}\")\n",
    "\n",
    "for i in range(8):  # Show first 8 filters\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(filters[i, 0], cmap='gray')\n",
    "    plt.title(f'Filter {i+1}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('First Layer CNN Filters')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are designed for sequential data. Let's create a simple LSTM network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"Simple LSTM for sequence classification\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Use output from last time step\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = SimpleLSTM(input_size=10, hidden_size=64, \n",
    "                       num_layers=2, num_classes=3)\n",
    "print(\"LSTM Architecture:\")\n",
    "print(lstm_model)\n",
    "\n",
    "# Test with synthetic sequence data\n",
    "batch_size = 4\n",
    "seq_length = 20\n",
    "input_size = 10\n",
    "\n",
    "test_sequences = torch.randn(batch_size, seq_length, input_size)\n",
    "print(f\"\\nInput shape: {test_sequences.shape}\")\n",
    "print(f\"(batch_size, sequence_length, input_features)\")\n",
    "\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = lstm_model(test_sequences)\n",
    "    \n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output logits:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Topics\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "Transfer learning allows us to use pre-trained models and adapt them for our specific tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning example (requires torchvision)\n",
    "try:\n",
    "    import torchvision.models as models\n",
    "    \n",
    "    # Load pre-trained ResNet-18\n",
    "    print(\"Loading pre-trained ResNet-18...\")\n",
    "    resnet = models.resnet18(pretrained=True)\n",
    "    \n",
    "    print(f\"Original final layer: {resnet.fc}\")\n",
    "    \n",
    "    # Freeze all parameters\n",
    "    for param in resnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace final layer for our number of classes\n",
    "    num_classes = 5\n",
    "    resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
    "    \n",
    "    print(f\"\\nModified final layer: {resnet.fc}\")\n",
    "    print(f\"Modified for {num_classes} classes\")\n",
    "    \n",
    "    # Show which parameters will be updated\n",
    "    params_to_update = []\n",
    "    for name, param in resnet.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            params_to_update.append(param)\n",
    "            print(f\"Parameter to update: {name}\")\n",
    "    \n",
    "    print(f\"\\nTotal parameters to update: {len(params_to_update)}\")\n",
    "    print(\"Only the final classification layer will be trained!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"torchvision not available - transfer learning example skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Optimizers\n",
    "\n",
    "PyTorch provides various optimization algorithms. Let's compare their behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different optimizers on a simple optimization problem\n",
    "def rosenbrock(x, y):\n",
    "    \"\"\"Rosenbrock function - a classic optimization test function\"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "# Test different optimizers\n",
    "optimizers_to_test = {\n",
    "    'SGD': lambda params: optim.SGD(params, lr=0.001),\n",
    "    'Adam': lambda params: optim.Adam(params, lr=0.01),\n",
    "    'RMSprop': lambda params: optim.RMSprop(params, lr=0.01),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for idx, (opt_name, opt_func) in enumerate(optimizers_to_test.items()):\n",
    "    # Initialize parameters\n",
    "    x = torch.tensor([-1.5], requires_grad=True)\n",
    "    y = torch.tensor([2.0], requires_grad=True)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = opt_func([x, y])\n",
    "    \n",
    "    # Track optimization path\n",
    "    loss_history = []\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        loss = rosenbrock(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "    \n",
    "    # Plot results\n",
    "    plt.subplot(1, 3, idx + 1)\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(f'{opt_name} Optimization')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    print(f\"{opt_name:8} - Final loss: {loss_history[-1]:.6f}, x: {x.item():.3f}, y: {y.item():.3f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOptimal solution is at x=1, y=1 with loss=0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving and Loading\n",
    "\n",
    "It's important to know how to save and load trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving and loading\n",
    "print(\"=== Model Saving and Loading ===\")\n",
    "\n",
    "# Create a simple model\n",
    "original_model = SimpleNN(10, 64, 3)\n",
    "\n",
    "# Method 1: Save only the state dictionary (recommended)\n",
    "torch.save(original_model.state_dict(), 'model_state_dict.pth')\n",
    "print(\"Model state dict saved as 'model_state_dict.pth'\")\n",
    "\n",
    "# Load the state dictionary\n",
    "loaded_model = SimpleNN(10, 64, 3)  # Must create model with same architecture\n",
    "loaded_model.load_state_dict(torch.load('model_state_dict.pth', weights_only=True))\n",
    "loaded_model.eval()\n",
    "print(\"Model state dict loaded successfully\")\n",
    "\n",
    "# Test that models are identical\n",
    "test_input = torch.randn(1, 10)\n",
    "\n",
    "original_model.eval()\n",
    "with torch.no_grad():\n",
    "    output1 = original_model(test_input)\n",
    "    output2 = loaded_model(test_input)\n",
    "    \n",
    "print(f\"\\nOriginal model output: {output1}\")\n",
    "print(f\"Loaded model output:   {output2}\")\n",
    "print(f\"Models produce identical outputs: {torch.allclose(output1, output2)}\")\n",
    "\n",
    "# Method 2: Save complete model (less flexible)\n",
    "torch.save(original_model, 'complete_model.pth')\n",
    "loaded_complete = torch.load('complete_model.pth', weights_only=False)\n",
    "print(\"\\nComplete model saved and loaded\")\n",
    "\n",
    "# Save training checkpoint (including optimizer state)\n",
    "optimizer = optim.Adam(original_model.parameters(), lr=0.001)\n",
    "checkpoint = {\n",
    "    'epoch': 100,\n",
    "    'model_state_dict': original_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': 0.05,\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "print(\"Training checkpoint saved\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('checkpoint.pth', weights_only=False)\n",
    "model_from_checkpoint = SimpleNN(10, 64, 3)\n",
    "optimizer_from_checkpoint = optim.Adam(model_from_checkpoint.parameters(), lr=0.001)\n",
    "\n",
    "model_from_checkpoint.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_from_checkpoint.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "print(f\"Checkpoint loaded: epoch {epoch}, loss {loss}\")\n",
    "print(\"Ready to resume training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **PyTorch Fundamentals**\n",
    "   - Creating and manipulating tensors\n",
    "   - Automatic differentiation with autograd\n",
    "\n",
    "2. **Neural Networks**\n",
    "   - Building models with nn.Module\n",
    "   - Forward pass and parameter counting\n",
    "\n",
    "3. **Training**\n",
    "   - Complete training loop with loss, backpropagation, and optimization\n",
    "   - Model evaluation and metrics\n",
    "   - Visualizing training progress\n",
    "\n",
    "4. **Advanced Architectures**\n",
    "   - Convolutional Neural Networks for images\n",
    "   - Recurrent Neural Networks for sequences\n",
    "\n",
    "5. **Advanced Topics**\n",
    "   - Transfer learning with pre-trained models\n",
    "   - Comparing different optimizers\n",
    "   - Model saving and loading\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment**: Try modifying the architectures and hyperparameters\n",
    "- **Real Data**: Apply these techniques to real datasets (MNIST, CIFAR-10, etc.)\n",
    "- **Advanced Topics**: Explore GANs, Transformers, and other advanced architectures\n",
    "- **Production**: Learn about model deployment and optimization\n",
    "\n",
    "### Key Mathematical Concepts Covered\n",
    "\n",
    "- **Gradient Descent**: Î¸ = Î¸ - Î±âˆ‡J(Î¸)\n",
    "- **Chain Rule**: Essential for backpropagation\n",
    "- **Convolution**: Feature extraction in images\n",
    "- **LSTM Gates**: Managing information flow in sequences\n",
    "\n",
    "Happy deep learning with PyTorch! ðŸ”¥ðŸ§ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}